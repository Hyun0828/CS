# 목차
- [I/O Devices](#36-i/o-devices)
  - [표준 장치](#표준-장치)
  - [표준 방식](#표준-방식)
  - [하드웨어 인터럽트](#하드웨어-인터럽트)
  - [DMA](#dma)
  - [디바이스와의 상호작용](#디바이스와의-상호작용)
  - [디바이스 드라이버](#디바이스-드라이버)
- [Hard Disk Drives](#37-hard-disk-drives)
  - [인터페이스](#인터페이스)
  - [기본 구조](#기본-구조)
  - [멀티 트랙](#멀티-트랙)
  - [I/O 시간 계산](#io-시간-계산)
  - [디스크 스케줄링](#디스크-스케줄링)
  - [SSTF : 최단 탐색 시간 우선](#sstf)
  - [엘리베이터](#엘리베이터)
  - [SPTF : 최단 위치 잡기 우선](#sptf) 
 
# Persistence

## 36. I/O Devices

I/O가 없는 시스템은 의미가 없다. 시스템에 I/O를 효율적으로 통합하는 과정을 살펴보자. 그 전에 우선 일반적인 시스템 구조를 살펴보자.

<img width="417" alt="스크린샷 2025-05-03 오후 7 55 09" src="https://github.com/user-attachments/assets/60439eee-e7b8-478d-8cfe-15f107f42daf" />

위 그림에서는 CPU와 메모리가 메모리 버스로 연결되어 있다. 그래픽이나 다른 고성능 I/O 장치들이 PCI 같은 범용 I/O 버스에 연결될 수 있다. 그 아래에는 SCSI나 SATA와 같은 주변 장치용 버스가 있다. 이 버스에는 가장 느린 장치들이 연결된다.

이런 계층적인 버스 구조를 사용하는 이유는 물리 공간적인 이유와 비용 때문이다. 버스 성능이 빨라지기 위해서는 버스가 짧아져야 하는데 그럼 여러 장치들을 수용할 공간적인 여유가 없다. 또한 고속의 버스를 만드는데 비용이 많이 들기 때문이다. 따라서 고성능 장치들을 CPU와 가깝게, 느린 장치들을 그보다 멀리 배치한 것이다. 

### 표준 장치

표준 장치는 다른 구성 요소에게 제공하는 하드웨어 인터페이스와 내부 장치가 있다. 내부 장치는 보통 시스템에게 제공하는 장치에 대한 추상화를 정의한다. 당연하겠지만 복잡한 장치 일수록 내부 구조가 복잡해진다.

<img width="363" alt="스크린샷 2025-05-03 오후 7 58 17" src="https://github.com/user-attachments/assets/1e68b2d7-ecf7-4f05-901f-588df2fda0e1" />

### 표준 방식

위에서 본 단순한 표준 장치의 인터페이스는 3개의 레지스터를 가지고 있다. 상태 레지스터는 장치의 현재 상태를 읽을 수 있으며 명령어 레지스터는 장치가 특정 동작을 수행하도록 요청할 때, 그리고 데이터 레지스터는 장치에 데이터를 보내거나 받을 때 사용한다. 이 3개의 레지스터를 통해 운영체제는 장치의 동작을 제어한다. 

기본적으로 장치는 어떻게 동작할까?

```java
while(status == BUSY)
; // 장치가 바쁜 상태가 아닐 때까지 대기
데이터를 DATA 레지스터에 쓰기
명령어를 COMMAND 레지스터에 쓰기, 이후 장치가 명령어를 실행
while(status == BUSY)
; // 장치가 바쁜 상태가 아닐 때까지 대기
```

위와 같이 동작하는데 장치의 상태를 계속 확인하는 작업인 Polling이 문제가 된다. 다른 프로세스에게 CPU를 양도하지 안고 계속 루프를 돌면서 장치 상태를 체크하고 있다. 이전에 멀티 쓰레드 환경에서 Spin Lock에서 발생한 문제와 동일한 것 같다. 이를 어떻게 해결할까?

### 하드웨어 인터럽트

디바이스를 폴링하는 대신 장치가 작업을 끝마치면 하드웨어 인터럽트를 발생시켜 CPU는 운영체제가 미리 정의해놓은 인터럽트 서비스 루틴이나 인터럽트 핸들러를 실행시킨다. 인터럽트 핸들러는 입출력 요청의 완료, I/O를 대기 중인 프로세스를 깨우는 등의 기능을 한다. 

<img width="470" alt="스크린샷 2025-05-03 오후 8 06 52" src="https://github.com/user-attachments/assets/2001a1d5-d4fb-4202-9055-56c0c2cf90a5" />

위의 경우에 p는 Polling 과정을 의미한다. 즉 인터럽트가 없다면 시스템은 I/O가 종료될 때 까지 반복적으로 장치의 상태를 확인한다.

<img width="476" alt="image" src="https://github.com/user-attachments/assets/1713c86e-44f4-49e8-b178-1948dc600946" />

반면에 인터럽트를 사용하면 CPU가 디스크 I/O 작업 동안 다른 프로세스를 처리할 수 있다. 그러나 이런 인터럽트 방식이 항상 좋은 것은 아니다. I/O가 매우 빠르게 처리되는 경우에는 오히려 폴링이 유리하다. 왜냐면 인터럽트 방식은 프로세스간의 문맥 교환이 발생하는데 문맥 교환은 사실 비용이 비싼 작업이기 때문이다. 그래서 장치의 속도가 빠르면 폴링을, 느리면 인터럽트를 사용하는 것이 좋지만 애매하거나 모르는 경우에는 둘을 같이 사용하는 하이브리드 방식도 사용한다.

만약 인터럽트를 사용하는 경우에는 각 요청이 끝날 때 인터럽트를 발생시키기 보다는 버퍼처럼 인터럽트를 한 번에 모아 CPU를 전달하는 병합 기법을 사용한다고 한다.

### DMA

만약 많은 양의 데이터를 디스크로 전달하기 위해 데이터 전송에 CPU가 직접 관여하는 Programmed I/O를 사용하면 CPU가 또 다시 사용된다. 

<img width="460" alt="image (1)" src="https://github.com/user-attachments/assets/aa238ea9-85a1-40a5-92d1-e880e7e400c7" />

위와 같이 프로세스 1에서 디스크에 데이터를 기록하려고 할 때 데이터를 메모리에서 디스크로 전송하는 과정을 CPU가 처리하게 된다. 이 문제를 DMA(Direct Memory Access) 장치가 해결한다.

<img width="467" alt="image (2)" src="https://github.com/user-attachments/assets/5256bd52-5959-4a3e-af92-f8b79500c193" />

장치 간의 메모리 전송을 CPU가 아닌 DMA가 담당하게 만든다. DMA 엔진에 메모리 상의 데이터 위치와 전송할 데이터의 크기, 대상 장치 등을 프로그램 해두면 끝이다. DMA 동작이 끝나면 DMA 컨트롤러가 인터럽트를 발생시켜 전송이 완료되었음을 알린다.

### 디바이스와의 상호작용

장치와의 효율적인 상호작용이 중요함을 알았다. 그럼 장치와의 정보 교환은 어떻게 하는 것일까?

첫 번째는 가장 오래된 방법으로 I/O 명령을 명시적으로 사용하는 것이다. 예를 들어 x86의 경우에는 in과 out 명령어를 사용하여 장치들과 통신할 수 있다. 이 명령어들은 대부분 특권 명령어이기 때문에 운영체제만이 직접 장치들과 통신할 수 있다.

두 번째 방법은 memory mapped I/O를 사용하는 것이다. 간단하게 장치의 레지스터들이 메모리 상에 존재하는 것처럼 만들어 해당 주소에 load/store 명령어를 사용해 데이터를 주고 받는다. 

특별히 좋은 방법이 있는 것은 아니라 두 방법이 모두 쓰이고 있다고 한다.

### 디바이스 드라이버

장치와 데이터를 주고 받는 방법, 그 속에서 하드웨어 인터럽트를 이용해 효율적으로 CPU를 사용하는 법에 대해 알아보았다. 최종적으로 다룰 문제는 서로 다른 인터페이스를 갖는 장치들과 운영체제를 연결시키는 것이다. 파일 시스템을 예로 들어보면 SCSI 디스크와 USB 드라이브 등과 같은 기기 위에서 동작하는 파일 시스템을 만들어야 하는데 각 장치들의 구체적인 입출력 명령어 형식에 종속되지 않게 해야 하는 것이다.

이를 위해 추상화를 사용한다. 운영체제 최하위 계층의 일부 소프트웨어는 장치의 동작 방식을 알고 있다. 이 소프트웨어는 디바이스 드라이버라고 부르며 장치와의 상세한 상호작용은 그안에 캡슐화 되어 있다.

<img width="399" alt="image (3)" src="https://github.com/user-attachments/assets/f0049317-7f92-4473-a490-e8000dfde7b6" />

위 그림은 Linux의 파일 시스템 소프트웨어 계층이다. 파일 시스템은 디스크 종류를 전혀 모르는 상태에서 범용 블록 계층에 블럭 read/write 요청을 전달한다. 범용 블록 계층은 이를 적절한 디바이스 드라이버로 전달하며 디바이스 드라이버는 특정 요청을 장치에 내리기 위해 필요한 작업을 한다. 즉 우리가 코드를 작성할 때도 읽어, 써, 닫아, 열어 등과 같은 간단한 API를 통해 각각의 장치를 사용할 수 있게 되는 것이다.

OSI 7계층에서 상위 프로토콜에선 하위 프로토콜의 인터페이스만 사용할 뿐 어떻게 작업을 처리하는 지 (비트 단위, 프레임 단위를 어떻게 전송하는 지) 등은 몰라도 되는 것과 흡사한 듯..

## 37. Hard Disk Drives

I/O 장치 중 대표적인 하드 디스크 드라이브에 대해 살펴보자. 디스크가 어떻게 데이터를 저장하고 처리하는 지, 디스크 스케줄링은 뭔지 알아보자

### 인터페이스

이전에 표준 장치들은 인터페이스를 갖고 있다고 했다. 현대 디스크 드라이브의 인터페이스를 이해해보자. 드라이브는 읽고 쓸수 있는 많은 수의 섹터(512byte 블럭)들로 이루어져 있다. 디스크 위의 n개의 섹터들은 0~n-1 개의 이름이 붙어 있어 사실 섹터들의 배열로 볼 수 있다 멀티 섹터 작업도 충분히 가능하지만 많은 드라이브는 하나의 섹터 쓰기만 원자성을 보장하기에 대량의 쓰기 중에 일부만 완료될 수도 있다.

### 기본 구조

- 플래터 : 원형의 표면에 자기적 성질을 변형하여 데이터를 지속시킨다. 플래터는 2개의 표면을 갖고 있으며 비트를 영구적으로 저장하기 위해 얇은 자성층이 입혀져 있다. 회전축으로 고정되어 있다.
- 트랙 : 표면의 동심원 하나를 트랙이라고 부른다.
- 표면 위를 읽거나 쓸 때 디스크 암에 연결되어 있는 디스크 헤드를 통해 이루어진다.

### 멀티 트랙

<img width="497" alt="image" src="https://github.com/user-attachments/assets/a74e68c2-c1ff-482f-b8a7-ec64399c1e67" />

왼쪽처럼 현재 30번에서 11번 섹터를 읽는 경우라고 해보자. 이 읽기 요청을 처리하기 위해서 우선 디스크 암을 올바른 트랙위에 위치시킨다. 이 과정을 **탐색(seek)** 이라고 부른다. 탐색 과정 중에 플래터는 당연하게도 회전한다. 오른쪽 그림이 잘못 나와 있는 것 같은데 탐색 이후 9번 섹터가 헤드 밑에 있다면 11번 섹터까지는 조금만 회전해도 도달할 수 있다. 섹터 11번이 헤드를 지나가게 되면 마지막 단계인 **전송**이 이루어져 표면 위의 데이터를 읽거나 쓰게 된다. 그 외에 트랙 비틀림, 멀티 구역 디스크 드라이브, 캐시 등의 개념들도 있다. 

캐시는 디스크에서 하나의 섹터를 읽을 때 해당 데이터를 저장해둬 후에 섹터를 읽을 때 빠른 응답을 제공할 수 있다. 쓰기의 경우에 메모리에 데이터가 기록된 시점에 쓰기가 완료됨을 알리는 write-back과 디스크에 실제로 기록되었을 때 완료됨을 알리는 write-through 방식이 있다. 보통 write-back 성능이 더 우수하지만 **특정 순서로 디스크에 기록해야 한다고 할 때는** write-back 방식은 문제가 생길 수 있다.

### I/O 시간 계산

<img width="255" alt="image (1)" src="https://github.com/user-attachments/assets/b9373aed-faab-4195-9268-842edf94bdc9" />

위의 식을 이용해 I/O 시간을 계산할 수 있다. 디스크 작업으로는 랜덤 워크로드와 순차 워크로드가 있는데 둘다 흔히들 사용된다. 

<img width="318" alt="스크린샷 2025-05-03 오후 9 18 08" src="https://github.com/user-attachments/assets/bd44bec3-5872-4f8d-bc33-1b38c779d53a" />
<img width="309" alt="image (2)" src="https://github.com/user-attachments/assets/2245deed-0ade-461d-b3c4-9e89d84303de" />

성능이 우수한 디스크와 용량이 큰 디스크를 이용해 I/O 시간을 비교해보자. 랜덤 워크로드가 순차 워크로드보다 훨씬 느리다. 디스크간의 비교에서 랜덤 워크로드에선 성능이 우수한 디스크에서 2배 정도 우수한 성능을 보였다. 그러나 순차 워크로드에선 큰 차이를 보이진 않았다. 

### 디스크 스케줄링

I/O의 비용이 크기 때문에 여러 I/O 요청이 발생했을 때 어떤 요청을 먼저 처리할 지는 굉장히 중요하다. 우리가 **프로세스 스케줄링을 배울 때는 각 작업이 얼마나 걸릴 지를 알 수 없었지만 디스크 스케줄링은 각 요청이 얼마나 걸릴 지를 꽤 정확하게 예측할 수 있다.** 따라서 greedy하게 가장 빠른 요청을 선택할 수 있고 이는 SJF의 원칙을 따른다고 할 수 있다.

### SSTF

**SSTF(Shortest Seek Time First)는** 가장 가까운 트랙의 요청이 우선 처리되도록 한다. 그러나 드라이브 구조는 호스트 운영체제에게 공개되어 있지 않는 문제가 있다. 운영체제는 블럭들의 배열로만 인식하기 때문이다. 그래서 이를 해결하기 위해 SSTF 대신 가장 가까운 블럭 우선 방식인 **NBF(Nearest-block-first)** 방식을 사용한다. 

그러나 가까운 안쪽 트랙에만 지속적으로 요청이 발생하면 **기아** 현상이 발생한다. SSTF 방식에선 이 문제를 해결할 수 없다.

### 엘리베이터

SCAN 또는 C-SCAN이라고도 부르는 엘리베이터 방식이 있는데 디스크를 한 번 가로지르는 것을 (밖에서 안으로 또는 안에서 밖으로) **스위프(sweep)** 라고 부른다. 어떤 요청이 이번 스위프에 이미 지나간 트랙에 대해 들어온다면 다음 번 반대 방향 스위프에서 처리되도록 큐에서 대기한다.

엘리베이터 방식은 기아 현상을 해결할 수 있다. 그러나 SJF 원칙을 지키지는 못한다. 탐색은 고려하지만 회전을 고려하지 않기 때문이다.

### SPTF

SPTF(Shortest Positioning Time First)로 해결할 수 있다.  탐색과 회전을 모두 고려한 총 위치 시간이 가장 짧은 요청을 먼저 처리한다.

<img width="283" alt="스크린샷 2025-05-03 오후 9 28 09" src="https://github.com/user-attachments/assets/f0af106f-f0f6-4a9b-ad30-1a15aff9dd45" />

스케줄러는 8번과 16번 섹터에 대한 요청 중 어떤 요청을 먼저 처리할까? 당연히 상황에 따라 다르다. 탐색이 회전보다 빠르면 먼 탐색을 먼저 처리하는 것이 낫다. 16번 섹터는 8번 섹터에 비해 회전 시간이 길기 때문이다. 반대로 탐색이 느리면 16번을 먼저 처리한다. 

그러나 현대 드라이브는 탐색과 회전이 대부분 비슷하기 때문에 이 두 요소를 전부 고려하는 SPTF가 유용하게 동작한다. 하지만 트랙의 경계가 어디인지, 현재 헤드가 어디에 있는지를 운영체제에서 구현하기는 매우 어렵기 때문에 실질적으로는 디스크 드라이브 내부에서 실행된다. 그리고 엘리베이터에 비해 SPTF는 기아 현상에 취약하다고 한다.

디스크 스케줄러가 수행하는 또 다른 중요한 작업은 **I/O 병합이다.** 위 그림에서 33번, 8번, 34번 요청이 발생했다고 하면 3번의 I/O를 처리하기 보단 33번과 34번을 묶어 순차적으로 처리하려 요청을 재배치한다. 

또한 디스크로 I/O를 내려보내기 전에 시스템은 얼마나 기다려야 하는가? 단 하나의 I/O만 있더라도 디스크로 즉시 내려보낼 수도 있지만 때로는 “좀 더 좋은” 요청이 디스크에 도달할 수도 있기 마련이다. 그러나 얼마나 기다려야 할 지 결정하는 것은 매우 까다롭다.
