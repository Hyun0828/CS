# 목차
- [I/O Devices](#36-io-devices)
  - [표준 장치](#표준-장치)
  - [표준 방식](#표준-방식)
  - [하드웨어 인터럽트](#하드웨어-인터럽트)
  - [DMA](#dma)
  - [디바이스와의 상호작용](#디바이스와의-상호작용)
  - [디바이스 드라이버](#디바이스-드라이버)
- [Hard Disk Drives](#37-hard-disk-drives)
  - [인터페이스](#인터페이스)
  - [기본 구조](#기본-구조)
  - [멀티 트랙](#멀티-트랙)
  - [I/O 시간 계산](#io-시간-계산)
  - [디스크 스케줄링](#디스크-스케줄링)
  - [SSTF : 최단 탐색 시간 우선](#sstf)
  - [엘리베이터](#엘리베이터)
  - [SPTF : 최단 위치 잡기 우선](#sptf)
- [RAID](#38-raid)
  - [RAID 0](#raid-0)
  - [RAID 0의 성능](#raid-0의-성능)
  - [RAID 1](#raid-1)
  - [RAID 1의 성능](#raid-1의-성능)
  - [RAID 4](#raid-4)  
  - [RAID 4의 성능](#raid-4의-성능)
  - [RAID 5](#raid-5)
  - [RAID 6](#raid-6)
  - [RAID 비교](#raid-비교)
- [Files and Directories](#39-files-and-directories)
  - [파일과 디렉터리](#파일과-디렉터리)
  - [파일의 생성](#파일의-생성)
  - [파일의 읽기와 쓰기](#파일의-읽기와-쓰기)
  - [비 순차적 읽기와 쓰기](#비-순차적-읽기와-쓰기)
  - [즉시 기록](#즉시-기록)
  - [파일 이름 변경](#파일-이름-변경)
  - [파일 정보 추출](#파일-정보-추출)
  - [파일 삭제](#파일-삭제)
  - [디렉터리 생성](#디렉터리-생성)
  - [디렉터리 읽기](#디렉터리-읽기)
  - [디렉터리 삭제](#디렉터리-삭제)
  - [하드 링크](#하드-링크)
  - [심볼릭 링크](#심볼릭-링크)
  - [마운트](#마운트)
- [File System Implementation](#40-file-system-implementation)
  - [전체 구성](#전체-구성)
  - [아이노드](#아이노드)
  - [멀티 레벨 인덱스](#멀티-레벨-인덱스)
  - [디렉터리 구조](#디렉터리-구조)
  - [빈 공간 관리](#빈-공간-관리)
  - [파일 읽기](#파일-읽기)
  - [파일 쓰기](#파일-쓰기)
  - [캐싱과 버퍼링](#캐싱과-버퍼링) 
    
# Persistence

## 36. I/O Devices

I/O가 없는 시스템은 의미가 없다. 시스템에 I/O를 효율적으로 통합하는 과정을 살펴보자. 그 전에 우선 일반적인 시스템 구조를 살펴보자.

<img width="417" alt="스크린샷 2025-05-03 오후 7 55 09" src="https://github.com/user-attachments/assets/60439eee-e7b8-478d-8cfe-15f107f42daf" />

위 그림에서는 CPU와 메모리가 메모리 버스로 연결되어 있다. 그래픽이나 다른 고성능 I/O 장치들이 PCI 같은 범용 I/O 버스에 연결될 수 있다. 그 아래에는 SCSI나 SATA와 같은 주변 장치용 버스가 있다. 이 버스에는 가장 느린 장치들이 연결된다.

이런 계층적인 버스 구조를 사용하는 이유는 물리 공간적인 이유와 비용 때문이다. 버스 성능이 빨라지기 위해서는 버스가 짧아져야 하는데 그럼 여러 장치들을 수용할 공간적인 여유가 없다. 또한 고속의 버스를 만드는데 비용이 많이 들기 때문이다. 따라서 고성능 장치들을 CPU와 가깝게, 느린 장치들을 그보다 멀리 배치한 것이다. 

### 표준 장치

표준 장치는 다른 구성 요소에게 제공하는 하드웨어 인터페이스와 내부 장치가 있다. 내부 장치는 보통 시스템에게 제공하는 장치에 대한 추상화를 정의한다. 당연하겠지만 복잡한 장치 일수록 내부 구조가 복잡해진다.

<img width="363" alt="스크린샷 2025-05-03 오후 7 58 17" src="https://github.com/user-attachments/assets/1e68b2d7-ecf7-4f05-901f-588df2fda0e1" />

### 표준 방식

위에서 본 단순한 표준 장치의 인터페이스는 3개의 레지스터를 가지고 있다. 상태 레지스터는 장치의 현재 상태를 읽을 수 있으며 명령어 레지스터는 장치가 특정 동작을 수행하도록 요청할 때, 그리고 데이터 레지스터는 장치에 데이터를 보내거나 받을 때 사용한다. 이 3개의 레지스터를 통해 운영체제는 장치의 동작을 제어한다. 

기본적으로 장치는 어떻게 동작할까?

```java
while(status == BUSY)
; // 장치가 바쁜 상태가 아닐 때까지 대기
데이터를 DATA 레지스터에 쓰기
명령어를 COMMAND 레지스터에 쓰기, 이후 장치가 명령어를 실행
while(status == BUSY)
; // 장치가 바쁜 상태가 아닐 때까지 대기
```

위와 같이 동작하는데 장치의 상태를 계속 확인하는 작업인 Polling이 문제가 된다. 다른 프로세스에게 CPU를 양도하지 안고 계속 루프를 돌면서 장치 상태를 체크하고 있다. 이전에 멀티 쓰레드 환경에서 Spin Lock에서 발생한 문제와 동일한 것 같다. 이를 어떻게 해결할까?

### 하드웨어 인터럽트

디바이스를 폴링하는 대신 장치가 작업을 끝마치면 하드웨어 인터럽트를 발생시켜 CPU는 운영체제가 미리 정의해놓은 인터럽트 서비스 루틴이나 인터럽트 핸들러를 실행시킨다. 인터럽트 핸들러는 입출력 요청의 완료, I/O를 대기 중인 프로세스를 깨우는 등의 기능을 한다. 

<img width="470" alt="스크린샷 2025-05-03 오후 8 06 52" src="https://github.com/user-attachments/assets/2001a1d5-d4fb-4202-9055-56c0c2cf90a5" />

위의 경우에 p는 Polling 과정을 의미한다. 즉 인터럽트가 없다면 시스템은 I/O가 종료될 때 까지 반복적으로 장치의 상태를 확인한다.

<img width="476" alt="image" src="https://github.com/user-attachments/assets/1713c86e-44f4-49e8-b178-1948dc600946" />

반면에 인터럽트를 사용하면 CPU가 디스크 I/O 작업 동안 다른 프로세스를 처리할 수 있다. 그러나 이런 인터럽트 방식이 항상 좋은 것은 아니다. I/O가 매우 빠르게 처리되는 경우에는 오히려 폴링이 유리하다. 왜냐면 인터럽트 방식은 프로세스간의 문맥 교환이 발생하는데 문맥 교환은 사실 비용이 비싼 작업이기 때문이다. 그래서 장치의 속도가 빠르면 폴링을, 느리면 인터럽트를 사용하는 것이 좋지만 애매하거나 모르는 경우에는 둘을 같이 사용하는 하이브리드 방식도 사용한다.

만약 인터럽트를 사용하는 경우에는 각 요청이 끝날 때 인터럽트를 발생시키기 보다는 버퍼처럼 인터럽트를 한 번에 모아 CPU를 전달하는 병합 기법을 사용한다고 한다.

### DMA

만약 많은 양의 데이터를 디스크로 전달하기 위해 데이터 전송에 CPU가 직접 관여하는 Programmed I/O를 사용하면 CPU가 또 다시 사용된다. 

<img width="460" alt="image (1)" src="https://github.com/user-attachments/assets/aa238ea9-85a1-40a5-92d1-e880e7e400c7" />

위와 같이 프로세스 1에서 디스크에 데이터를 기록하려고 할 때 데이터를 메모리에서 디스크로 전송하는 과정을 CPU가 처리하게 된다. 이 문제를 DMA(Direct Memory Access) 장치가 해결한다.

<img width="467" alt="image (2)" src="https://github.com/user-attachments/assets/5256bd52-5959-4a3e-af92-f8b79500c193" />

장치 간의 메모리 전송을 CPU가 아닌 DMA가 담당하게 만든다. DMA 엔진에 메모리 상의 데이터 위치와 전송할 데이터의 크기, 대상 장치 등을 프로그램 해두면 끝이다. DMA 동작이 끝나면 DMA 컨트롤러가 인터럽트를 발생시켜 전송이 완료되었음을 알린다.

### 디바이스와의 상호작용

장치와의 효율적인 상호작용이 중요함을 알았다. 그럼 장치와의 정보 교환은 어떻게 하는 것일까?

첫 번째는 가장 오래된 방법으로 I/O 명령을 명시적으로 사용하는 것이다. 예를 들어 x86의 경우에는 in과 out 명령어를 사용하여 장치들과 통신할 수 있다. 이 명령어들은 대부분 특권 명령어이기 때문에 운영체제만이 직접 장치들과 통신할 수 있다.

두 번째 방법은 memory mapped I/O를 사용하는 것이다. 간단하게 장치의 레지스터들이 메모리 상에 존재하는 것처럼 만들어 해당 주소에 load/store 명령어를 사용해 데이터를 주고 받는다. 

특별히 좋은 방법이 있는 것은 아니라 두 방법이 모두 쓰이고 있다고 한다.

### 디바이스 드라이버

장치와 데이터를 주고 받는 방법, 그 속에서 하드웨어 인터럽트를 이용해 효율적으로 CPU를 사용하는 법에 대해 알아보았다. 최종적으로 다룰 문제는 서로 다른 인터페이스를 갖는 장치들과 운영체제를 연결시키는 것이다. 파일 시스템을 예로 들어보면 SCSI 디스크와 USB 드라이브 등과 같은 기기 위에서 동작하는 파일 시스템을 만들어야 하는데 각 장치들의 구체적인 입출력 명령어 형식에 종속되지 않게 해야 하는 것이다.

이를 위해 추상화를 사용한다. 운영체제 최하위 계층의 일부 소프트웨어는 장치의 동작 방식을 알고 있다. 이 소프트웨어는 디바이스 드라이버라고 부르며 장치와의 상세한 상호작용은 그안에 캡슐화 되어 있다.

<img width="399" alt="image (3)" src="https://github.com/user-attachments/assets/f0049317-7f92-4473-a490-e8000dfde7b6" />

위 그림은 Linux의 파일 시스템 소프트웨어 계층이다. 파일 시스템은 디스크 종류를 전혀 모르는 상태에서 범용 블록 계층에 블럭 read/write 요청을 전달한다. 범용 블록 계층은 이를 적절한 디바이스 드라이버로 전달하며 디바이스 드라이버는 특정 요청을 장치에 내리기 위해 필요한 작업을 한다. 즉 우리가 코드를 작성할 때도 읽어, 써, 닫아, 열어 등과 같은 간단한 API를 통해 각각의 장치를 사용할 수 있게 되는 것이다.

OSI 7계층에서 상위 프로토콜에선 하위 프로토콜의 인터페이스만 사용할 뿐 어떻게 작업을 처리하는 지 (비트 단위, 프레임 단위를 어떻게 전송하는 지) 등은 몰라도 되는 것과 흡사한 듯..

## 37. Hard Disk Drives

I/O 장치 중 대표적인 하드 디스크 드라이브에 대해 살펴보자. 디스크가 어떻게 데이터를 저장하고 처리하는 지, 디스크 스케줄링은 뭔지 알아보자

### 인터페이스

이전에 표준 장치들은 인터페이스를 갖고 있다고 했다. 현대 디스크 드라이브의 인터페이스를 이해해보자. 드라이브는 읽고 쓸수 있는 많은 수의 섹터(512byte 블럭)들로 이루어져 있다. 디스크 위의 n개의 섹터들은 0~n-1 개의 이름이 붙어 있어 사실 섹터들의 배열로 볼 수 있다 멀티 섹터 작업도 충분히 가능하지만 많은 드라이브는 하나의 섹터 쓰기만 원자성을 보장하기에 대량의 쓰기 중에 일부만 완료될 수도 있다.

### 기본 구조

- 플래터 : 원형의 표면에 자기적 성질을 변형하여 데이터를 지속시킨다. 플래터는 2개의 표면을 갖고 있으며 비트를 영구적으로 저장하기 위해 얇은 자성층이 입혀져 있다. 회전축으로 고정되어 있다.
- 트랙 : 표면의 동심원 하나를 트랙이라고 부른다.
- 표면 위를 읽거나 쓸 때 디스크 암에 연결되어 있는 디스크 헤드를 통해 이루어진다.

### 멀티 트랙

<img width="497" alt="image" src="https://github.com/user-attachments/assets/a74e68c2-c1ff-482f-b8a7-ec64399c1e67" />

왼쪽처럼 현재 30번에서 11번 섹터를 읽는 경우라고 해보자. 이 읽기 요청을 처리하기 위해서 우선 디스크 암을 올바른 트랙위에 위치시킨다. 이 과정을 **탐색(seek)** 이라고 부른다. 탐색 과정 중에 플래터는 당연하게도 회전한다. 오른쪽 그림이 잘못 나와 있는 것 같은데 탐색 이후 9번 섹터가 헤드 밑에 있다면 11번 섹터까지는 조금만 회전해도 도달할 수 있다. 섹터 11번이 헤드를 지나가게 되면 마지막 단계인 **전송**이 이루어져 표면 위의 데이터를 읽거나 쓰게 된다. 그 외에 트랙 비틀림, 멀티 구역 디스크 드라이브, 캐시 등의 개념들도 있다. 

캐시는 디스크에서 하나의 섹터를 읽을 때 해당 데이터를 저장해둬 후에 섹터를 읽을 때 빠른 응답을 제공할 수 있다. 쓰기의 경우에 메모리에 데이터가 기록된 시점에 쓰기가 완료됨을 알리는 write-back과 디스크에 실제로 기록되었을 때 완료됨을 알리는 write-through 방식이 있다. 보통 write-back 성능이 더 우수하지만 **특정 순서로 디스크에 기록해야 한다고 할 때는** write-back 방식은 문제가 생길 수 있다.

### I/O 시간 계산

<img width="255" alt="image (1)" src="https://github.com/user-attachments/assets/b9373aed-faab-4195-9268-842edf94bdc9" />

위의 식을 이용해 I/O 시간을 계산할 수 있다. 디스크 작업으로는 랜덤 워크로드와 순차 워크로드가 있는데 둘다 흔히들 사용된다. 

<img width="318" alt="스크린샷 2025-05-03 오후 9 18 08" src="https://github.com/user-attachments/assets/bd44bec3-5872-4f8d-bc33-1b38c779d53a" />
<img width="309" alt="image (2)" src="https://github.com/user-attachments/assets/2245deed-0ade-461d-b3c4-9e89d84303de" />

성능이 우수한 디스크와 용량이 큰 디스크를 이용해 I/O 시간을 비교해보자. 랜덤 워크로드가 순차 워크로드보다 훨씬 느리다. 디스크간의 비교에서 랜덤 워크로드에선 성능이 우수한 디스크에서 2배 정도 우수한 성능을 보였다. 그러나 순차 워크로드에선 큰 차이를 보이진 않았다. 

### 디스크 스케줄링

I/O의 비용이 크기 때문에 여러 I/O 요청이 발생했을 때 어떤 요청을 먼저 처리할 지는 굉장히 중요하다. 우리가 **프로세스 스케줄링을 배울 때는 각 작업이 얼마나 걸릴 지를 알 수 없었지만 디스크 스케줄링은 각 요청이 얼마나 걸릴 지를 꽤 정확하게 예측할 수 있다.** 따라서 greedy하게 가장 빠른 요청을 선택할 수 있고 이는 SJF의 원칙을 따른다고 할 수 있다.

### SSTF

**SSTF(Shortest Seek Time First)는** 가장 가까운 트랙의 요청이 우선 처리되도록 한다. 그러나 드라이브 구조는 호스트 운영체제에게 공개되어 있지 않는 문제가 있다. 운영체제는 블럭들의 배열로만 인식하기 때문이다. 그래서 이를 해결하기 위해 SSTF 대신 가장 가까운 블럭 우선 방식인 **NBF(Nearest-block-first)** 방식을 사용한다. 

그러나 가까운 안쪽 트랙에만 지속적으로 요청이 발생하면 **기아** 현상이 발생한다. SSTF 방식에선 이 문제를 해결할 수 없다.

### 엘리베이터

SCAN 또는 C-SCAN이라고도 부르는 엘리베이터 방식이 있는데 디스크를 한 번 가로지르는 것을 (밖에서 안으로 또는 안에서 밖으로) **스위프(sweep)** 라고 부른다. 어떤 요청이 이번 스위프에 이미 지나간 트랙에 대해 들어온다면 다음 번 반대 방향 스위프에서 처리되도록 큐에서 대기한다.

엘리베이터 방식은 기아 현상을 해결할 수 있다. 그러나 SJF 원칙을 지키지는 못한다. 탐색은 고려하지만 회전을 고려하지 않기 때문이다.

### SPTF

SPTF(Shortest Positioning Time First)로 해결할 수 있다.  탐색과 회전을 모두 고려한 총 위치 시간이 가장 짧은 요청을 먼저 처리한다.

<img width="283" alt="스크린샷 2025-05-03 오후 9 28 09" src="https://github.com/user-attachments/assets/f0af106f-f0f6-4a9b-ad30-1a15aff9dd45" />

스케줄러는 8번과 16번 섹터에 대한 요청 중 어떤 요청을 먼저 처리할까? 당연히 상황에 따라 다르다. 탐색이 회전보다 빠르면 먼 탐색을 먼저 처리하는 것이 낫다. 16번 섹터는 8번 섹터에 비해 회전 시간이 길기 때문이다. 반대로 탐색이 느리면 16번을 먼저 처리한다. 

그러나 현대 드라이브는 탐색과 회전이 대부분 비슷하기 때문에 이 두 요소를 전부 고려하는 SPTF가 유용하게 동작한다. 하지만 트랙의 경계가 어디인지, 현재 헤드가 어디에 있는지를 운영체제에서 구현하기는 매우 어렵기 때문에 실질적으로는 디스크 드라이브 내부에서 실행된다. 그리고 엘리베이터에 비해 SPTF는 기아 현상에 취약하다고 한다.

디스크 스케줄러가 수행하는 또 다른 중요한 작업은 **I/O 병합이다.** 위 그림에서 33번, 8번, 34번 요청이 발생했다고 하면 3번의 I/O를 처리하기 보단 33번과 34번을 묶어 순차적으로 처리하려 요청을 재배치한다. 

또한 디스크로 I/O를 내려보내기 전에 시스템은 얼마나 기다려야 하는가? 단 하나의 I/O만 있더라도 디스크로 즉시 내려보낼 수도 있지만 때로는 “좀 더 좋은” 요청이 디스크에 도달할 수도 있기 마련이다. 그러나 얼마나 기다려야 할 지 결정하는 것은 매우 까다롭다.

## 38. RAID

우리는 대용량이면서 빠르고 신뢰할 수 있는 디스크를 원한다. 이를 위해 여러 개의 디스크를 조화롭게 사용하는 디스크 시스템인 RAID가 등장했다. 외면적으로는 하나의 디스크처럼 보이나 여러 개의 디스크와 메모리, 시스템을 관리하는 하나 또는 그 이상의 프로세서로 이루어진 복잡한 장치이다. 

상위 파일 시스템에게는 RAID가 크고 빠르고 신뢰할 수 있는 큰 디스크로 보인다. 파일 시스템이 RAID에 논리적 I/O를 요청하면 RAID는 내부에서 하나 또는 그 이상의 물리적 I/O를 발생시킨다. 

RAID를 구성하는 방법은 여러 가지가 있는데 각 방법의 장점과 단점을 이해하기 위해서는 평가 기준이 필요하다. 사용할 수 있는 용량, 디스크 결함을 감내할 수 있는 신뢰성, 성능 이 3가지가 중요하다.

### RAID 0

RAID 레벨 0 또는 스트라이핑 이라고 알려진 이 방식은 사실 중복 저장을 하지 않기 때문에 RAID 레벨은 아니다. 

<img width="452" alt="image" src="https://github.com/user-attachments/assets/abdf876e-cc9a-4cbc-8c7e-9ef723654352" />
<img width="512" alt="스크린샷 2025-05-03 오후 11 04 04" src="https://github.com/user-attachments/assets/5418be9f-aa5d-4be3-ba60-042ef4ee38a8" />

디스크 배열의 블럭들을 라운드 로빈 방식으로 디스크를 가로질러 펼치는 방법이다. 같은 행의 블럭들을 스트라이프 라고 부른다. 왼쪽 기준으로 블럭 0,1,2,3은 같은 스트라이프에 존재한다. 그런데 꼭 이렇게 배치할 필요는 없다.

<img width="511" alt="image (1)" src="https://github.com/user-attachments/assets/8f4402bc-9324-4d22-9df9-9da6b5261b4b" />

위와 같이 2개의 블럭을 배치한 후에 다음 디스크로 넘어가도 되며 청크 크기를 조절하면 된다.

이 청크 크기는 RAID 성능에 큰 영향을 준다. 청크 크기가 작으면 많은 파일들이 여러 디스크에 걸쳐서 스트라이프 된다는 것이며 하나의 파일을 읽고 쓸 때 병렬성이 증가한다. 그러나 블럭의 위치를 여러 디스크에서 찾아야 하므로 위치 찾기 시간이 늘어난다. 반면 청크 크기가 크면 병렬성은 줄어들고 위치 찾기 시간이 감소하는 장점이 있다. 

### RAID 0의 성능

RAID-0은 N개의 디스크에서 NxB개의 디스크 용량만큼 유효 용량을 갖는다. 그러나 신뢰성 측면에선 최악이다. 하나의 디스크라도 고장나면 전체 데이터가 손실된다. 마지막으로 성능은 좋다. 모든 I/O를 병렬적으로 처리할 수 있기 때문이다.

RAID-0에서 한 블럭에 대한 요청 지연 시간은 하나의 디스크에 대한 요청 지연 시간과 같다. 받은 요청을 하나의 디스크로 보내주기만 하면 되기 때문이다. 처리 성능은 랜덤 I/O의 경우 N(디스크 개수)xR(랜덤 접근 대역폭), 순차 I/O의 경우 NxS(순차 접근 대역폭)과 같다. 다른 RAID에 비해 최대 대역폭을 기대할 수 있다.

### RAID 1

RAID-1은 미러링으로 알려져 있다. 각 블럭에 대해 하나 이상의 사본을 두어 디스크 고장에 대처할 수 있다.

<img width="303" alt="image" src="https://github.com/user-attachments/assets/5f7b386f-2cc1-4134-bb80-13b4b80cc725" />
<img width="518" alt="스크린샷 2025-05-03 오후 11 12 00" src="https://github.com/user-attachments/assets/d113ea2e-f305-47ee-bddd-2efdef52dd2e" />

미러링 방식은 다양한데 미러링 후 스트라이핑을 적용하냐(RAID 1+0), 스트라이핑 후 미러링을 적용하냐(RAID 0+1) 등 다양한 방식이 있다. 왼쪽은 전자 방식이다. 

블럭을 읽을 때 원본을 읽을 지, 사본을 읽을 지를 선택할 수 있지만 쓰는 경우에는 사본까지 전부 갱신해야 한다. 그러나 쓰기 요청은 병렬적으로 처리될 수 있다는 것을 기억하자.

### RAID 1의 성능

용량 측면에선 RAID-0의 절반이 된다. 그러나 신뢰성 측면에선 꽤 괜찮다. 디스크 1개의 고장은 무조건 감내할 수 있고 운이 좋으면 N/2개의 디스크 고장까지도 감내할 수 있다. 성능을 분석해보면 단일 읽기 요청의 지연 시간은 단일 디스크의 지연 시간과 동일하다. 읽기 요청을 원본과 사본 디스크 중 1개로 전달하면 되기 때문이다. 그러나 쓰기 요청은 2개의 디스크에 모두 전달되어야 하는데 이는 병렬 처리가 가능하지만 2개의 요청 중 최악의 요청 시간은 평균적으로 하나의 디스크 요청 시간보단 조금 길다.

순차 워크로드에선 각 논리 쓰기에 대해 2번의 물리 쓰기가 필요하다. 따라서 최대 대역폭의 절반인 NxS/2를 얻을 수 있다. 

블럭 : 0, 1, 2, 3, 4, 5, 6, 7

디스크 : 0, 2, 1, 3, 0, 2, 1, 3

위와 같이 저장되어 있다고 해보자. 순차 읽기에서 디스크 0을 살펴보면 블록 0을 읽고 다음에 블록 4를 읽어야 한다. 문제가 없어 보이지만 디스크에서 블록 0과 블록 4는 물리적으로 연속되어 저장되어 있지 않다. 따라서 디스크 0,1,2,3은 병렬로 읽을 수 있지만 각 디스크에서는 헤드가 다음 블록으로 이동할 때까지 기다리면서 지연 시간이 발생하는데 각 디스크는 최대 대역폭의 반만 쓸 수 있다. NxS/2

(GPT한테 물어봤는데 확실하지 않음.. 솔직히 잘 모르겠음;)

랜덤 읽기는 NxR, 랜덤 쓰기는 2번의 물리 쓰기가 필요하기 때문에 NxR/2 대역폭을 갖는다.

### RAID 4

신뢰성을 보장하기 위해 복사본을 저장하지 않고 패리티 비트를 사용한다. RAID-1에 비해 유효 용량이 훨씬 커지지만 성능은 오히려 줄어든다. 

<img width="362" alt="image (2)" src="https://github.com/user-attachments/assets/f39c447d-d0db-43d9-b470-4adf4c4c57e2" />
<img width="531" alt="image (1)" src="https://github.com/user-attachments/assets/2c09ef96-5ab8-4398-aee0-3cf79b464bd8" />

각 스트라이프 마다 패리티 블럭 하나를 패리티 디스크에 추가한다. 패리티는 간단하게 ‘1’이 짝수가 되게 만들어 주는 비트이다. (XOR을 이용해 만든다)

<img width="316" alt="image" src="https://github.com/user-attachments/assets/2456997f-fe56-4de5-8a9e-57ab5cf8bcdc" />

각 블럭의 각 비트 별로 패리티 비트를 계산해서 패리티 디스크에 저장하면 된다.

### RAID 4의 성능

패리티 디스크를 제외한 디스크는 전부 사용 가능하기에 (N-1)xB이다. 오직 하나의 디스크 고장을 감내할 수 있으나 2개 이상의 디스크 고장은 방법이 없다. 마지막으로 성능을 보면 나머지 경우에는 패리티 디스크를 제외한 디스크를 전부 사용 가능하다. 그런데 랜덤 쓰기의 경우에는 만약 블럭 1번을 갱신하려는 경우에 패리티 블럭 P0도 갱신 되어야 한다. 이를 효율적으로 갱신하는 방법은 2가지가 있다.

- 가산적 패리티 : 스트라이프 내의 다른 데이터 블럭을 병렬적으로 읽어 새 패리티 값을 구해 새 데이터와 함께 병렬적으로 쓴다. 이 기법의 문제는 디스크 개수가 많아지면 계산 양이 많아진다는 것이다.
- 감산적 패리티 : 데이터의 옛날 값과 새로운 값이 동일하면 패리티 값이 유지되며 다르면 패리티 비트를 현재 상태의 반대 값으로 뒤집는다.(0→1, 1→0)

<img width="361" alt="스크린샷 2025-05-03 오후 11 56 37" src="https://github.com/user-attachments/assets/c1648499-610d-45c6-8db5-1277556b7294" />

만약 4번과 13번을 갱신하는 요청이 거의 동시에 들어온다고 해보자. 디스크 0과 1은 병렬적으로 읽고 쓸 수 있지만 패리티 디스크는 그렇지 않다. 모든 쓰기 요청은 패리티 디스크로 인해 순차적으로 처리된다. 또한 패리티 디스크는 논리 I/O 당 읽기 1회와 쓰기 1회의 두 번의 I/O를 처리해야 하기 때문에 절반의 성능을 보인다. 이를 **small-write** 문제라고 한다. 이렇게 RAID-4에선 패리티 디스크가 병목이 된다.

### RAID 5

<img width="357" alt="image" src="https://github.com/user-attachments/assets/36273d18-f80a-4818-a4dc-6ebdeaff62b1" />
<img width="430" alt="스크린샷 2025-05-04 오전 12 01 23" src="https://github.com/user-attachments/assets/ad23a7a1-4ba1-455e-b88b-138cff2ecbfe" />

RAID-4에서의 패리티 디스크 병목 현상을 없애기 위해 패리티 블록을 전체 디스크에 걸쳐 순환 배치시킨다. RAID-4의 성능이 좋은 몇 가지 예외를 제외하면 RAID-5의 성능이 기본적으로 동일하기 때문에 완전히 대체하였다. 

### RAID 6

<img width="512" alt="image (1)" src="https://github.com/user-attachments/assets/22039b05-03e7-4c5c-b06f-c633af713451" />

RAID-5와 기본적으로 같지만 서로 다른 2개의 패리티 블럭을 두는 방식으로 안전성이 높지만 패리티 블럭을 2개 관리하기 때문에 쓰기 속도가 느리다.

### RAID 비교

<img width="530" alt="image (2)" src="https://github.com/user-attachments/assets/c7b7a696-58d4-4926-8864-34a17c5441ae" />

신뢰성이 중요하지 않고 성능만 중요하면 RAID-0을, 신뢰성과 성능을 중요시 하면 RAID-1을, 신뢰성과 용량을 중요시 하면 RAID-5를 선택해야 한다. 물론 RAID-5는 Small Write 비용을 일부 지불해야 한다. 트특정 워크로드에 적합한 기법을 고르면 된다.

---

책에서 순차, 랜덤 워크로드 별로 대역폭 설명을 해줬는데 솔직히 음… 이해가 잘 안 간다. 각 기법의 특징까진 이해하긴 했다.

## 39. Files and Directories

### 파일과 디렉터리

파일은 단순히 읽거나 쓸 수 있는 순차적인 바이트의 배열이다. 각 파일은 저수준의 이름을 갖고 있는데 이를 **아이노드 번호**라고 하며 사용자는 알 수 없다. 디렉터리도 파일과 마찬가지로 아이노드 번호를 갖는다. 하지만 파일과는 달리 디렉터리는 <사용자가 읽을 수 있는 이름, 저수준의 이름> 쌍으로 이루어진 목록을 갖는다. 즉 디렉터리 내에 다른 디렉터리와 파일들의 목록 리스트를 갖고 있다고 생각하면 된다. 이를 통해 디렉터리 트리를 구성할 수 있다.

<img width="343" alt="스크린샷 2025-05-05 오후 4 52 56" src="https://github.com/user-attachments/assets/117c4cbb-73d1-4010-83d3-769f65717609" />

위와 같이 디렉터리 트리/계층은 루트 디렉터리 ‘/’로 부터 시작하여 구분자를 사용해 하위 디렉터리를 명시할 수 있다. 파일은 파일 이름과 확장자로 이루어져 있는데 확장자가 .c라고 해서 반드시 c 소스 코드일 필요는 없다. 

이제 파일, 디렉터리와 관련한 시스템 콜을 살펴보자

### 파일의 생성

파일의 생성은 open 시스템 콜을 사용한다. 

```c
int fd = open("foo", O_CREAT | O_WRONLY | O_TRUNC);
```

- O_CREATE : 파일 생성 플래그
- O_WRONLY : 파일이 열렸을 때 쓰기만 가능한 플래그
- O_TRUNC : 기존 내용을 삭제하는 플래그

open의 중요한 부분은 리턴 값인데 이를 **파일 디스크립터(File Descriptor)** 라고 한다. 파일 디스크립터는 UNIX 시스템에서 파일 접근에 사용되며 해당 파일에 대한 특정 동작의 수행 자격을 부여하는 핸들이다. 

### 파일의 읽기와 쓰기

```bash
prompt > echo hello > foo
prompt > cat foo
hello
prompt > 
```

foo 라는 파일에 hello를 출력하고 cat으로 파일 내용을 확인했다. 어떤 시스템 콜이 호출되는 지는 Linux의 strace를 사용하면 된다.

```bash
prompt> strace cat foo
. . .
open(“foo ”, O_RDONLY|O_LARGEFILE) = 3
read(3,“hello\n ”, 4096) = 6
write(1,“hello\n ”, 6) = 6
hello
read(3,“ ”, 4096) = 0
close(3) = 0
. . .
prompt>
```

open을 통해 파일을 열고 리턴된 파일 디스크립터를 read 시스템 콜에서 사용한다. **이 때 open의 리턴 값이 3인 이유가 뭘까?** **기본적으로 표준 입력, 표준 출력, 표준 에러의 파일 디스크립터 값이 0,1,2 이기 때문이다.** 어쨌든 파일을 열고 read 시스템 콜을 통해 파일에서 몇 바이트씩 읽어온다. write에는 파일 디스크립터 값을 1로 두어 표준 출력을 진행한다. 출력 후 더 읽을 게 없으면 close가 호출된다.

### 비 순차적 읽기와 쓰기

파일의 특정 부분만 읽고 싶을 수도 있다. 이 때 lseek() 시스템 콜을 사용한다.

```c
off_t lseek(int fildes, off_t offset, int whence);
```

첫 인자는 파일 디스크립터이며 whence 플래그에 따라 파일 읽는 위치가 달라진다.

- SEEK_SET : 오프셋은 offset 바이트로 설정된다.
- SEEK_CUR : 오프셋은 현재 위치 + offset으로 설정된다.
- SEEK_END : 오프셋은 파일의 크기 + offset으로 설정된다.

### 즉시 기록

디스크 접근은 상당히 느리기 때문에 write() 시스템 콜이 호출되더라도 즉시 기록되지 않으며 버퍼링 과정이 존재한다. 그런데 디스크에 버퍼 내용이 기록되기 전에 문제가 생기면 데이터가 유실될 수 있다. 따라서 DB에서의 Flush 처럼 즉시 디스크에 기록할 수 있는 시스템 콜이 필요한데 이것이 **fsync(int fd)** 이다. fsync를 호출하면 특정 파일 디스크립터에 대한 모든 더티가 즉시 디스크에 기록된다.

여기서 주의할 점은 파일이 존재하는 디렉터리도 fsync()를 해주어야 한다는 것이다.

### 파일 이름 변경

우리는 mv 명령으로 파일 이름을 변경할 수 있고 rename(char *old, char *new) 시스템 콜을 호출한다. rename은 원자성을 보장받는 다는 것이 특징이다. 예시를 보자.

```c
int fd = open(“foo.txt.tmp ”, O_WRONLY|O_CREAT|O_TRUNC);
write(fd, buffer, size); // 파일᮹ ᔩಽᬕ ქᱥᮥ ᥑʑ
fsync(fd);
close(fd);
rename(“foo.txt.tmp ”, “foo.txt ”);
```

위와 같이 임시 파일에 데이터를 쓰고 rename을 호출하면 이전 버전의 파일을 삭제하고 동시에 새로운 파일로 교체하는 작업이 원자적으로 이루어진다.

### 파일 정보 추출

파일에 대한 정보를 메타데이터 라고 하는데 이 메타데이터를 보려면 **stat(), fstat()** 시스템 콜을 호출한다. 

```c
struct stat {
	dev_t st_dev; /* ID of device containing file */
	ino_t st_ino; /* inode number */
	mode_t st_mode; /* protection */
	nlink_t st_nlink; /* number of hard links */
	uid_t st_uid; /* user ID of owner */
	gid_t st_gid; /* group ID of owner */
	dev_t st_rdev; /* device ID (if special file) */
	off_t st_size; /* total size, in bytes */
	blksize_t st_blksize; /* blocksize for filesystem I/O */
	blkcnt_t st_blocks; /* number of blocks allocated */
	time_t st_atime; /* time of last access */
	time_t st_mtime; /* time of last modification */
	time_t st_ctime; /* time of last status change */
};
```

메타데이터는 아이노드 번호, 접근 시간, 변경 시간 등을 저장하고 있다. 일반적으로 파일 시스템은 아이노드에 메타데이터를 보관한다. **즉 아이노드는 파일의 메타데이터를 저장하는 디스크 자료구조라고 보면 된다.**

### 파일 삭제

파일 삭제 명령어인 rm은 unlink() 시스템 콜을 호출하는데 우선 디렉터리 관련 시스템 콜을 이해해보자.

### 디렉터리 생성

디렉터리 생성을 위한 시스템 콜은 **mkdir()**이다. 처음 디렉터리가 생성되면 빈 상태이지만 실제로는 자신을 나타내기 위한 항목인 ‘.’과 자신의 부모 디렉터리를 가리키는 ‘..’이 존재한다.

### 디렉터리 읽기

디렉터리 읽기는 **opendir(), readdir(), closedir()** 시스템 콜을 호출한다. 

```c
int main(int argc, char *argv[]) {
	DIR *dp = opendir(“ . ”);
	assert(dp != NULL);
	struct dirent *d;
	while ((d = readdir(dp)) != NULL) {
		printf(“%d %s\n ”, (int) d−>d_ino, d−>d_name);
	}
	closedir(dp);
	return 0;
}
```

struct dirent 자료구조는 각 디렉터리 항목에 저장된 정보를 보여준다. 즉 위 코드는 현재 디렉터리에 포함된 디렉터리와 파일들의 아이노드와 이름을 출력한다. 애초에 디렉터리에는 파일과 달리 이름과 아이노드 번호 매핑 목록을 제외하면 많은 정보가 없다.

### 디렉터리 삭제

**rmdir()** 시스템 콜을 호출하여 디렉터리를 삭제할 수 있는데 디렉터리를 삭제하기 전에 비워야 한다. 비어있지 않은 디렉터리는 호출이 실패한다. 

### 하드 링크

그럼 이전에 파일 삭제시 왜 unlink() 시스템 콜이 호출 되었을까? 파일 시스템 트리에 항목을 추가하는 link() 시스템 콜을 알아보자. link()는 원래 파일에 접근할 수 있는 새로운 이름인 하드 링크를 생성한다. 

```bash
prompt> echo hello > file
prompt> cat file
hello
prompt> ln file file2
prompt> cat file2
hello
```

file2 하드링크로 file에 접근이 가능해졌다. 이 말은 동일한 아이노드 번호에 대한 링크가 생성되었다는 것이다.

```bash
prompt> ls −i file file2
67158084 file
67158084 file2
prompt>
```

파일을 생성할 때는 사실 2가지 작업을 한다. 하나는 파일 메타데이터를 저장할 아이노드를 만드는 것이며 두 번째는 해당 파일에 사람이 읽을 수 있는 이름을 연결하고 그 연결 정보를 디렉터리에 생성하는 것이다. 파일 삭제 시 unlink()를 호출하는데 위의 예제에서 file을 제거한다고 하더라도 file2를 이용해 접근할 수 있다.

각 아이노드 번호에는 참조 횟수가 존재하는데 하드 링크가 생성되면 참조 횟수가 증가하며 파일이 삭제되거나 하드 링크가 삭제되면 참조 횟수가 줄어들어 0이 되면 파일이 실질적으로 삭제된다.

### 심볼릭 링크

심볼릭 링크는 소프트 링크라고도 부른다. 하드 링크는 디렉터리에 만들 수 없고 다른 디스크 파티션에 있는 파일에 대해서도 만들 수 없다는 제약이 있어 심볼릭 링크가 만들어지게 되었다. 심볼릭 링크는 기존 ln 명령어에 -s 플래그를 전달하면 된다.

```bash
prompt> echo hello > file
prompt> ln −s file file2
prompt> cat file2
hello
```

표면적으로는 하드 링크와 유사하지만 사실 매우 다르다. 심볼릭 링크는 파일 시스템에서 파일, 디렉터리와 같이 다른 형식의 독립된 파일이다. 

```bash
prompt> stat file
. . . regular file . . .
prompt> stat file2
. . . symbolic link . . .
```

심볼릭 링크는 연결하는 파일의 경로명을 저장한다. 파일의 경로명이 길수록 심볼릭 링크의 크기도 커진다. 그런데 이 심볼릭 링크는 **dangling reference** 문제가 발생할 수 있다. 

```bash
prompt> echo hello > file
prompt> ln −s file file2
prompt> cat file2
hello
prompt> rm file
prompt> cat file2
cat: file2: No such file or directory
```

하드 링크와 다르게 원본 파일을 삭제하면 심볼릭 링크가 가리키는 파일은 더 이상 존재하지 않게 된다.

### 마운트

다수의 파일 시스템이 존재할 때 이들을 묶어서 어떻게 큰 파일 디렉터리 트리를 구성할 수 있을까? 각각의 파일 시스템을 마운트하면 된다. 기본적으로 파일 시스템의 생성은 mkfs라는 도구를 사용한다. 장치명과 파일 시스템 타입을 전달하면 해당 파티션에 전달된 파일 시스템 형식으로 구성된 빈 파일 시스템을 생성한다. 이 파일 시스템은 자체적인 디렉터리 구조로 구성되어 있다. 

이렇게 새롭게 생성된 파일 시스템을 루트 디렉터리에서 시작하는 기존의 디렉터리 구성을 통해 접근할 수 있도록 해주어야 하는데 mount() 시스템 콜을 사용하여 내부적으로 처리하고 있다. 간단하게 기존 디렉터리 중 하나를 마운트 지점으로 지정해 생성된 파일 시스템을 붙여 넣는다.

예를 들어, 파티션 /dev/sda1에 EXT3 형식의 파일 시스템이 존재하고 루트 디렉터리 밑에 a, b의 2개 하위 디렉터리가 존재하며 각 디렉터리에는 foo 라는 파일이 하나 들어 있다. 이 파일 시스템을 /home/users 위치에 마운트해보자.

```bash
prompt> mount -t ext3 /dev/sda1 /home/users
```

마운트 작업이 성공하면 기존의 디렉터리 경로를 통해 접근할 수 있다.

```bash
prompt> ls /home/users/
a b
```

마운트를 이용하면 모든 파일 시스템들을 하나의 트리 아래에 통합시킬 수 있다. 우리가 PC에 USB를 연결하면 USB 내의 디렉터리 및 파일들을 PC 디렉터리 경로를 통해 접근할 수 있는 것도 마운트 덕분이다.

## 40. File System Implementation

가장 간단한 파일 시스템(vsfs) 구현에 대해 알아보자.. 사실 파일 시스템을 구현하는 방법들은 매우 다양하고 서로 다른 자료 구조를 갖고 있으며 각자 장단점도 있다. 그렇지만 전부 다루기엔 시간이 없으니 기본 개념을 공부하는 것으로..

우선 파일 시스템에 대해 이해하기 위해서는 자료구조와 접근 방법이 가장 중요하다. 데이터와 여러 파일의 메타 데이터를 어떤 종류의 자료구조로 저장해야 할까? 간단한 배열부터 좀 더 복잡한 트리 기반의 자료 구조까지 다양하다. 또한 프로세스가 여러 시스템 콜을 호출할 때 각 자료 구조를 어떻게 접근하고 사용할까? 이 2가지를 정확히 이해해보자

### 전체 구성

vsfs에서는 디스크를 **블럭** 단위로 나눈다. 일반적으로 블럭은 4KB 크기이다. N개의 블럭을 갖는 파티션에서 블럭은 0~N-1의 주소를 갖는다. 

<img width="499" alt="image" src="https://github.com/user-attachments/assets/255bc335-ce23-4bee-8610-d028500b8ceb" />

우선 사용자 데이터를 저장해야 할 것이다. 사용자 데이터가 저장된 디스크 공간을 **데이터 영역** 이라고 한다. 간단하게 56개의 블럭을 데이터 영역으로 사용한다고 하자.

파일 자체 데이터 이외에 메타 데이터도 존재한다. 메타 데이터는 어디에 저장할까? 앞에서 아이노드에 저장한다고 배웠다. 아이노드를 위한 디스크 공간인 **아이노드 테이블** 도 할당하자. 아이노드의 배열 형태로 구성된다. 5개의 블럭을 아이노드 테이블이 사용한다. 

일반적으로 아이노드는 128~256bytes 크기이며 256bytes라고 하면 한 블럭에 16개의 아이노드가 저장된다. 아이노드 테이블의 크기가 5블럭이면 아이노드의 개수는 80개이며 해당 디스크에 저장될 수 있는 파일의 개수가 80개임을 의미한다. 최대 파일 개수를 늘리려면 아이노드 테이블의 크기를 늘리면 된다.

우리는 추가로 각 아이노드와 데이터 영역의 블럭이 사용 중인지 아닌 지를 나타내는 정보 또한 저장해야 하는데 free list인 연결 리스트 형태로 저장하는 방법도 있지만 우리는 단순한 비트맵을 사용해 저장하자. 데이터 블럭의 사용 여부는 **데이터 비트맵** 으로, 아이노드의 사용 여부는 **아이노드 비트맵** 을 사용한다. 사용 중이면 1, 아니면 0으로 기록한다.

사실 비트맵에 1개 블럭을 전부 할당하는 건 공간 낭비다. 1개 블럭은 4KB = 32K bit이기 때문에 32K 개의 객체에 대한 관리가 가능하다. 그러나 이해를 위해 그냥 넘어가자.

여기까지 왔으면 64개의 블럭 중 1개 블럭이 남는데 이는 파일 시스템 전체에 대한 정보를 저장하는 **슈퍼 블럭** 이다. 대부분의 파일 시스템은 슈퍼 블럭을 몇 개 복사해둔다. 

파일 시스템을 마운트할 때, 해당 시스템의 슈퍼 블럭을 먼저 읽어서 작업을 진행한다.

<img width="503" alt="image (1)" src="https://github.com/user-attachments/assets/7a4c0a12-b7e6-4969-9352-414c597ca445" />

### 아이노드

각 아이노드는 아이노드 번호가 있으며 파일의 저수준 이름이라고 불렀다. vsfs에서는 아이노드 번호인 아이넘버를 이용해 각 아이노드가 디스크 상에 저장된 위치를 정확하게 계산할 수 있다. 

<img width="518" alt="image (2)" src="https://github.com/user-attachments/assets/7bac9871-7a8f-4b9a-8056-383859999dbc" />

위에서 32번 아이노드를 읽기 위해서는 아이노드 테이블 시작 주소 12KB + 32 x 아이노드 크기 256bytes를 하면 20KB가 나온다. 그런데 **디스크는 바이트 단위로 접근이 불가능하며 이를 섹터 주소로 변환해야 한다.** 그래서 대체적으로 사용되는 섹터 크기인 512bytes로 나누면 섹터주소 40을 계산할 수 있다.

아이노드에는 메타 데이터가 저장되어 있을 뿐만 아니라 파일의 데이터 블록 위치도 저장되어 있는데 가장 간단한 방법은 데이터 블록의 위치를 저장하는 **직접 포인터** 를 여러 개 두는 방법이다. **그런데 직접 포인터를 두면 직접 포인터 개수 x 블럭 크기로 파일 크기가 제한된다.** 예를 들어 아이노드에 직접 포인터가 12개가 있으면 아이노드가 가리키는 데이터 블럭은 12개일 것이며 파일 크기가 12x4KB=48KB로 제한된다.

### 멀티 레벨 인덱스

이 문제를 해결하기 위해 **간접 포인터** 가 등장했다. 간접 포인터는 데이터 블럭을 가리키지 않고 간접 포인터가 가리키는 블럭에는 데이터 블럭을 가리키는 직접 포인터들이 저장된다. 아이노드에 직접 포인터 12개와 간접 포인터 1개가 있다면 최대 파일의 크기는 (12 + 1024) x 4KB가 된다. 근데 이 값도 사실 꽤 작은 값이다. 따라서 이중 간접 포인터, 삼중 간접 포인터 등 파일의 최대 크기를 늘리기 위해 이 방법들을 사용할 수도 있다. 

이제까지 설명한 것을 종합해보면, 디스크 블럭은 편향된, 일종의 트리 형태로 구성되어 있다. 이를 **멀티 레벨 인덱스** 기법이라고 한다. (트리 구조 말고 디스크 포인터와 길이로 이루어진 익스텐트 방식도 있다) 그런데 재밌는 사실은 트리가 편향 구조라는 것이다. 이것은 타당한데 파일 시스템에 대한 오랜 연구 끝에 대부분의 파일 크기는 작다는 사실을 발견했고 작은 파일을 빨리 읽고 쓸 수 있도록 첫 12개의 블럭들은 직접 포인터를, 크기가 늘어날 수록 간접포인터, 이중 간접포인터를 사용하는 것이다. 

아이노드는 자료구조이기 때문에 더 효율적으로 파일을 읽고 쓸 수 있다면 트리 구조를 고집할 필요가 없다.

### 디렉터리 구조

디렉터리는 대부분 그렇듯 (파일 이름, 아이노드 번호) 쌍으로 구성되어 있다. 디렉터리의 데이터 블럭에는 문자열과 숫자가 쌍으로 존재하며 문자열 길이에 대한 정보도 있다. 예를 들어 dir 디렉터리에 foo, bar, foobar라는 3개의 파일이 있다면 데이터 블럭은 아래와 같이 구성된다.

<img width="277" alt="image (3)" src="https://github.com/user-attachments/assets/05c4c91f-e742-4c70-b30a-8bbcb4efcde2" />

디렉터리는 특수한 종류의 파일이다. 자신의 아이노드를 가지며 이는 파일과 같이 아이노드 테이블에 존재한다. 또한 자신의 데이터 블럭은 아이노드에 명시되어 있다. 디렉터리 항목들이 배열로 구성되어 있다면 파일이 존재하는 지 검사하는데 선형 시간이 걸린다. 그러나 B-tree와 같은 자료구조를 사용하면 검색 시간이 단축될 수 있다.

### 빈 공간 관리

파일 시스템은 아이노드와 데이터 블럭 사용 여부를 관리한다. 아까 언급했지만 이를 연결 리스트 구조로 관리할 수도 있다. 그러나 vsfs에서는 간단하게 비트맵을 사용하여 관리한다. 

### 파일 읽기

파일과 디렉터리를 저장하고 관리하는 방식은 비트맵, 아이노드 테이블, 데이터 블럭이다. 그럼 이 자료구조를 어떻게 접근해서 사용할까? 파일 시스템의 동작을 이해해보자.

간단한 예제로 단순히 파일을 열어 읽은 후에 닫는 상황을 가정해보자. 파일은 4KB 크기라고 해보자. 우리가 open(’/foo/bar’, ‘O_RDONLY) 시스템 콜을 호출하면 어떻게 동작할까?

1. 루트 디렉터리의 아이노드를 찾는다. 아이노드를 찾기 위해서는 아이노드 번호가 필요한데 루트의 아이노드 번호는 보통 잘 알려져 있으며 UNIX에선 2번이다. 아이노드 번호를 통해 아이노드를 읽는다.
2. 루트의 아이노드에서 데이터 블럭을 찾아 디렉터리 내용을 읽어 foo 항목을 찾는다. 이 때 디렉터리 크기가 크면 다수의 블럭을 읽어야 할 수도 있다. foo 파일의 아이노드 번호를 파악한다.
3. foo 파일의 아이노드 번호를 통해 아이노드를 찾는다. 똑같이 아이노드에서 데이터 블럭을 찾아 bar 항목을 찾고 bar 아이노드 번호를 파악한다.
4. bar 아이노드 번호르 통해 아이노드를 메모리로 읽어 들인다. 이후 open 시스템 콜은 파일 디스크립터를 리턴한다.

이후 read() 시스템 콜을 호출하면 메모리로 읽어 들인 bar의 아이노드를 통해 데이터 블럭을 찾으면 된다.

<img width="523" alt="image (4)" src="https://github.com/user-attachments/assets/d0a81054-be51-4ef4-b198-6541f7bffc8b" />

read()를 호출할 때는 아이노드를 읽고, 데이터 블럭을 읽은 후 아이노드에 마지막 접근 시간을 갱신한다. 이 때 디스크 I/O 횟수는 경로의 길이에 비례한다. 경로가 추가되면 해당 경로에 대한 아이노드와 데이터 블럭을 1번씩 더 읽어야 하기 때문이다. 

### 파일 쓰기

읽기와 다르게 파일 쓰기는 블럭 할당을 필요로 할 수 있다. 그래서 파일에 대한 쓰기 요청은 논리적으로 5번의 I/O를 생성한다. 

1. 데이터 비트맵을 읽고 데이터 블럭을 할당
2. 데이터 비트맵 갱신
3. 아이노드를 읽고
4. 아이노드에 데이터 블럭 정보 갱신
5. 데이터 블럭에 데이터를 기록

또한 파일 생성과 같은 단순 작업에서도 많은 양의 I/O가 발생한다. 

1. 아이노드 비트맵을 읽고
2. 아이노드 할당 정보를 비트맵에 갱신
3. 디렉터리의 데이터 블럭 갱신
4. 디렉터리 아이노드를 읽고 
5. 디렉터리의 아이노드를 갱신

이 때 만약 파일 생성으로 디렉터리 크기가 증가하면 디렉터리 데이터 블럭 할당을 위해 I/O가 2회 추가된다. 이렇게 간단한 작업에서도 대량의 I/O가 발생하는데 이를 어떻게 줄일 수 있을까?

### 캐싱과 버퍼링

성능 개선을 위해 파일 시스템들은 자주 사용되는 블럭들을 메모리에 캐싱한다. 초기 파일 시스템에서는 자주 사용되는 블럭을 저장하는 고정 크기의 캐시(메모리의 10%)를 사용했으나 메모리 낭비를 방지하기 위해 동적 파티션 방식으로 바뀌었다. 또한 현대의 많은 OS는 가상 메모리 페이지들과 파일 시스템 페이지들을 통합하여 일원화된 페이지 캐시를 만들었다. 

캐싱을 해두면 파일 열기의 경우에 첫 파일 열기는 I/O가 발생하겠지만 이후의 파일 열기는 캐시에서 히트 되기 때문에 성능 개선이 된다. 캐시가 충분히 크면 사실 대부분의 읽기 I/O를 제거할 수 있다. 근데 쓰기는 영속성을 유지하기 위해 해당 블럭들을 디스크로 내려야한다. 캐시는 쓰기 시점을 연기하는 쓰기 버퍼링 역할을 한다. 쓰기 버퍼링을 통해 얻을 수 있는 이점은 여러 가지가 있다.

- 쓰기 요청을 일괄처리하여 I/O 횟수 감소
    → 같은 연산(아이노드 비트맵 갱신)들은 병합 가능
- 다수의 I/O를 스케줄링하여 성능 개선
- 쓰기 자체를 피할 수 있다.
    → 파일을 생성한 후 삭제하면 디스크에 쓸 필요가 없다.

그러나 버퍼링은 디스크에 기록되기 전에 시스템이 크래시되면 내용이 손실되기 때문에 금융처럼 데이터 정합성이 중요한 경우에는 fsync()를 통해 데이터 유실을 피해야 한다.
